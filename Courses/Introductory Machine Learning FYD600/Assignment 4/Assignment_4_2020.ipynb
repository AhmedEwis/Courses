{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handout 4 (FYD600/GU): Deep Q-learning, Grid world\n",
    "### Authors: Jonatan WÃ¥rdh, Mats Granath, and Oleksandr Balabanov\n",
    "\n",
    "(2019, revised 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last homework we will continue on the previous project, the homework 3 Grid World, but we will now use a neural network to store the $Q$-matrix.\n",
    "\n",
    "In the last Homework we found a $Q$-matrix that described the values of taking the action up,down,left or right given the state we were in. The state was given by the position of the player which lived on a 20x30 grid corresponding to 600 possible states; the state space. The $Q$-matrix was pretty small and we had no problem storing all the values and go through them many times and update until the values converged. However, this smallness of the state space is more an exception than a general rule.\n",
    "\n",
    "In this assignment we will include a little twist to the game, a twist that makes the state space explode. What we will do is to let the fire spread with a certain probability after every turn of the player. For the sake of relative simplicity we will consider a 10x10 grid. The player has 100 possible positions but the fire can occupy any grid, or not, meaning that we have $100 \\cdot 2^{100} \\approx 10^{32}$ possible states. This is a huge number and there is no way of even storing this number of $Q$-matrix entries, even less so going through and updating all these values repeatedly. Of course, in principle the problem sounds quite simple. There should be no need to tune $10^{32}$ degrees of freedoms to learn the simple task of just avoiding the fire, but exactly how to capture that intuition in a self learning mathematical framework is less clear. (An alternative could be to use a rule based solution, but that has limited generalizability, and we do machine learning in this course.) This is where the neural network comes in. As we have seen in handout 1 and 2 a network has the possibility to learn general features of data. For the present problem it amounts to taking the state as an input and giving the value of the different actions as an output, i.e. we use the network to represent the $Q$-matrix. As you will see you will actually only need a very small number of parameters (compared to $10^{32}$) in your network to solve this problem.\n",
    "\n",
    "The ground breaking paper that popularized deep Q-learning: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "So what we need to alter from problem 3 is to implement the network as the $Q$-function. The update of the $Q$-function: $Q(s,a)\\leftarrow(1-\\alpha)Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a'))$, must then be replaced with the learning step of the network. This step meant that $r+\\gamma\\max_{a'}Q(s',a')$ was the new estimate of $Q(s,a)$, thus using the neural network we will then train on $r+\\gamma\\max_{a'}Q(s',a')$. \n",
    "\n",
    "So far everything seems like a straight forward generalization. However training the network is a quite tricky business. We will review a few problems and cures below. \n",
    "\n",
    "\n",
    "###  Catastrophic forgetting and experience replay\n",
    "\n",
    "The first problem one might encounter when using the network instead of simply going through all the values of the $Q$-matrix is that of catastrophic forgetting. This is easiest explained with an example; Let's assume that the player has the goal on its left hand side and the cliff on its right hand side. It now takes a step to the left and receive a reward for this, the network is then trained on this situation and learns to correlates some  feature of this state to the action of taking a step to the left. In the next game it might happen so that the player ends up with the goal to the right and the cliff to the left. With the network being trained on this the previous move it might see some common features of these two states and decides to make a move to the left, because it was what it had learned last time. However this time this results in a negative reward and is we now train on this event it is likely that the network erase what it previously learned. In this scenario we might go back and forth between these two events and not learn anything. \n",
    "\n",
    "Note that this could not happen if we had the complete $Q$ matrix entries for every single state, simply because the experiences are disjoint; updating one element of the $Q$-matrix will not effect any other value. However, for the network, training on one state will effect the output of another state. So the very property that make networks good for treating a large state space make them sensitive in this regard.\n",
    "\n",
    "So how do we solve this? Well we need to make the network learn to tell the qualitative difference between the two states and actions it apparently thought looked quite similar. What we do is then to train on both experiences simultaneously or at least repetively. In practice this is done by setting up a memory in which we store a certain number of the most recent experiences. When we come to the training we draw a random sample from this memory and train on it. This is called experience replay.\n",
    "\n",
    "### Instability : Policy and target networks\n",
    "\n",
    "Another issue is that the network may become unstable and the training might start to diverge. This problem comes partly from the fact that we use the network in order to predict future rewards, rewards that in the beginning are completely random. This means that the network will learn from its own prediction which can lead to a runaway situation. A cure for this is to use two networks: a _policy_ network and a _target_ network. These networks should have exactly the same architecture, they are just updated in different pace. The policy network determines the action of the player and is the network which is trained in the training step. The target network determines the target of the training, that is predicts $\\max_{a'}Q(s',a')$, and is updated less frequently. The target network is updated by copying the weights of the policy network and assigning them to the target network, so the target network is never trained, it is just a copy of the policy network. By delaying the feedback to the target values in the training step the instability might be avoided.\n",
    "\n",
    "\n",
    "# Assignment\n",
    "\n",
    "You will be provided with a code that implements the grid world game. Your task is to make a suitable network, and find suitable parameters. There is not a lot of coding required, but getting it to work may require a few attempts. Below follows a brief description of the code.\n",
    "\n",
    "\n",
    "## Code:\n",
    "\n",
    "The code defines a class GridWorld, if you need to read up on classes check [w3schools](https://www.w3schools.com/python/python_classes.asp) for a short version and [The Python tutorial](https://docs.python.org/2/tutorial/classes.html) for a more extensive one. However the main point of classes is that you treat objects (or instances) of that class. You do this by calling the constructor to get an object of the class, e.g. below we write <code>world = GridWorld()</code> to get the object <code>world</code> of the class. Now you can call any function of variable in the class by <code>world.variable</code> or <code>world.function(parameters)</code>.\n",
    "\n",
    "\n",
    "### State representation\n",
    "\n",
    "To represent the state we have used an array consisting of three 10x10 grid layers. The first specifies the position of the player with a 1 at the position of the player, otherwise zero, if the player has walked outside the grid all elements are zero. The second layer specifies the position of the fire, with 1 where there is a fire and 0 otherwise. The third layer represents the goal and is fixed at the same position. I.e. <code>state[:,:,0]</code> gives the grid describing the position of the player, <code>state[:,:,1]</code> position of the fire and <code>state[:,:,2]</code> position of the goal.\n",
    "\n",
    "## Questions and hints\n",
    "\n",
    "Questions:\n",
    "\n",
    "$\\bf{1}$   Design and train a Q-networks that solves the Grid World game with a fire of probability <code>prob_spread = 0.5</code> and <code>wind = 0</code> for a 10 by 10 grid. This is your main task, and required to pass. Plot the state value function for different configurations of the fire. Study also the dynamical play of the player. Success implies that the agent (most of the time) manages to move from start (position [1,8]) to goal (position [8,1]) while avoiding the fire.\n",
    "\n",
    "$\\bf{2}$ (Optional) Explore what happens if you increase <code>prob_spread</code> and include  <code>wind</code>.\n",
    "\n",
    "\n",
    "Below follows some hints. Note that the parameter values stated are not in any way correct values, it is just there to help you search in an appropriate parameter range\n",
    "\n",
    "$\\bullet$ Your network should not need more than 100000 parameters. But it might vary. Consider the activation function of the output nodes and the loss function.  \n",
    "\n",
    "$\\bullet$ To test the minimal requirements on your network you might want to try to disable the fire and perhaps also set <code>gamma = 0</code>, then then the network should learn the rewards of every square in the grid. \n",
    "\n",
    "$\\bullet$ The ouput diagnostic of q_max and q_min gives an indication whether the network output is reasonable or not. The range of Q-values should be in the range of possible returns. \n",
    "\n",
    "$\\bullet$ Make sure that use use big enough memory. We need experience from quite many games in the past, say 100, how many moves does the player do in each game? Considering this, how big should your memory then be. What should <code>batchSize</code> be? (Probably hard to say, trial and error may be needed.) \n",
    "\n",
    "$\\bullet$ In general <code>gamma</code> needs to be quite large in order for the player to see the goal from far away in the grid. If your training becomes unstable for large <code>gamma</code> you should consider having a bigger memory. You can also try to make the synchronization between policy and target networks less frequent. \n",
    "\n",
    "$\\bullet$ You will probably need to use a decay of <code>epsilon</code>, so that the player start out walking random but start listening more and more to the network. However, make sure that you do not quench <code>epsilon</code> to fast. A good idea might be to study how how the player is progressing as <code>epsilon</code> is lowered. If he never finds the goal when you reduced <code>epsilon</code> significantly, you probably reduces it to fast.\n",
    "\n",
    "$\\bullet$ You can consider wether to start at random position or at a fixed position, <code> random_start </code>, depending on how far you training has progressed. \n",
    "\n",
    "$\\bullet$ It might be a good idea to interrupt the training and assess how the learning progresses by studying the state value function. \n",
    "\n",
    "$\\bullet$ You will need to train for a few thousand games. However, you should be able to see progress after 1000 games.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and defining GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Libraries for plotting\n",
    "import matplotlib  \n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits import mplot3d\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# For safe copy of varaibles\n",
    "import copy\n",
    "\n",
    "# Nice way of building a memory, a list with maximum size\n",
    "from collections import deque\n",
    "import itertools\n",
    "\n",
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#Import the Keras layers etc\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout,BatchNormalization\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct Gridworld class\n",
    "class GridWorld:\n",
    "     \n",
    "    ##============ CONSTRUCTOR =============\n",
    "    # This creates the instance of the class and is called by GridWorld().\n",
    "    # The first argument in defining any function refers \n",
    "    # to the objects of the class currently handeled, often called self, but it could be anything.\n",
    "    # Note that when calling the functions these first argument is left out, i.e. you only write GridWorld(), \n",
    "    # the first argument is automatically fed by python.\n",
    "    def __init__(self):\n",
    "        # the size of the grid\n",
    "        self.size = np.array([10,10])\n",
    "        # number of layers in state \n",
    "        self.layers = 3\n",
    "        \n",
    "        # Default starting position and goal\n",
    "        self.start = np.array([1,8])\n",
    "        self.goalpos = np.array([8,1])      \n",
    "        \n",
    "        # rewards, gravel refers to an ordinary step \n",
    "        self.cliff = -100\n",
    "        self.fire = -50\n",
    "        self.goal = 100\n",
    "        self.gravel = -1\n",
    "                \n",
    "        # Default values for network\n",
    "        self.gamma = 0       \n",
    "        # Probability of wind\n",
    "        self.wind = 0.2\n",
    "        #probability for fire to spread\n",
    "        self.prob_spread = 0\n",
    "        \n",
    "        # Default values for epsilon greedy, not optimal, updated further down!\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.99999999999999999999999999\n",
    "        self.epsilon_min = 0.2\n",
    "        \n",
    "        # Memory, default values, not optimal, updated further down!\n",
    "        self.memory_size = 1 \n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        self.batchSize = 1 \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "##============ CREATE STATES =============\n",
    "    # Constructs the state, random_placement is either True or False. If True the player is placed \n",
    "    # randomly, if False the player is initialized in the starting position\n",
    "    \n",
    "    def make_state(self,random_placement):\n",
    "        \n",
    "        if random_placement:\n",
    "            r_x = np.random.randint(self.size[0]) \n",
    "            r_y = np.random.randint(self.size[1])\n",
    "            # if random = goal keep generating values \n",
    "            while r_x == self.goalpos[0] and r_y == self.goalpos[1]:\n",
    "                r_x = np.random.randint(self.size[0]) \n",
    "                r_y = np.random.randint(self.size[1])\n",
    "        else :\n",
    "            r_x = self.start[0]\n",
    "            r_y = self.start[1]\n",
    "        \n",
    "        # Initialize all values in all layers to zero\n",
    "        state = np.zeros((self.size[0],self.size[1],self.layers))\n",
    "        # we will use player_coordinate to keep track of the position of the player\n",
    "        player_coordinate = [0,0]\n",
    "\n",
    "        # Go through all layers and put 1 at the correct position\n",
    "        for x in range(self.size[0]) : \n",
    "            for y in range(self.size[1]) :\n",
    "            \n",
    "                # Player, first layer\n",
    "                if x == r_x and y == r_y :\n",
    "                    state[x,y,0] = 1\n",
    "                    player_coordinate[0] = x\n",
    "                    player_coordinate[1] = y\n",
    "                else : \n",
    "                    state[x,y,0] = 0\n",
    "\n",
    "                       \n",
    "                # Fire, second layer\n",
    "                if (1<= x <=2) and (1<= y <= 2):\n",
    "                    state[x,y,1] = 1    \n",
    "                else :\n",
    "                    state[x,y,1] = 0 \n",
    "            \n",
    "                #Goal, thrid layer\n",
    "                if x == self.goalpos[0] and y == self.goalpos[1] :\n",
    "                    state[x,y,2] = 1\n",
    "                else : \n",
    "                    state[x,y,2] = 0                \n",
    "    \n",
    "        # return state and player_coordinate\n",
    "        return state , player_coordinate \n",
    "    \n",
    "    \n",
    "##============ MAKING MOVES IN GRIDWORLD ================\n",
    " \n",
    "    # This function returns the new state, player position, reward of the move and a variable done\n",
    "    # which tells us if the game is done or not. The arguments are the current state, action, player_coordinate\n",
    "    # and is_wind which takes values True or False and determines if wind should be implemented.\n",
    "    \n",
    "    def make_move(self,state,action,player_coordinate,is_wind):\n",
    "        # Use deepcopy to make a copy of the state, otherwise this would just be a pointer to the same\n",
    "        # object as state. This is an inconvenience with python... \n",
    "        next_state = copy.deepcopy(state)\n",
    "                \n",
    "        if is_wind:\n",
    "            if np.random.rand() < self.wind:\n",
    "                # overwrite action with random action internally\n",
    "                action = np.random.randint(4)\n",
    "                \n",
    "        new_x = player_coordinate[0]\n",
    "        new_y = player_coordinate[1]\n",
    "        \n",
    "        # Assume that the player goes out of the board ,set old position to zero\n",
    "        # and new coordinate to none\n",
    "        next_state[new_x,new_y,0] = 0\n",
    "        next_player_coordinate = None\n",
    "        done = True\n",
    "        reward = self.cliff\n",
    "        \n",
    "        # make move \n",
    "        if action < 2 : # up or down\n",
    "            if action == 0: # up\n",
    "                new_y = new_y + 1\n",
    "            else : # down\n",
    "                new_y = new_y - 1 \n",
    "        else : # left or right\n",
    "            if action == 2: # left \n",
    "                new_x = new_x - 1\n",
    "            else : # right\n",
    "                new_x = new_x + 1   \n",
    "\n",
    "        # If inside grid \n",
    "        if 0<= new_x < self.size[0] and 0<= new_y < self.size[1] :\n",
    "            # if it hits the goal\n",
    "            if state[new_x,new_y,2] == 1:\n",
    "                done = True\n",
    "                reward = self.goal\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            # fire # WHAT IF THE GOAL BURNS?\n",
    "            elif state[new_x,new_y,1] == 1 :\n",
    "                done = False\n",
    "                reward = self.fire\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            # gravel    \n",
    "            else : \n",
    "                done = False \n",
    "                reward = self.gravel\n",
    "                next_state[new_x,new_y,0] = 1\n",
    "                next_player_coordinate = [new_x,new_y]\n",
    "            \n",
    "        # else, do nothing, next_player coordinate remains None and next_state[:,:,0] remains all zeros\n",
    "            \n",
    "        return next_state, next_player_coordinate , reward , done  \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "#============== SPREADING OF FIRE =====================\n",
    "\n",
    "    # This function takes state and returns, new_state in which the fire has spread. There is \n",
    "    # a possibility that the fire did not spread, therefore it gives did_fire_spread which is True if the \n",
    "    # fire actually did spread, otherwise it is False.\n",
    "    \n",
    "    def let_fire_spread(self,state):\n",
    "        new_state = copy.deepcopy(state)\n",
    "        \n",
    "        # Assume fire did not spread\n",
    "        did_fire_spread = False\n",
    "    \n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                # Walk trhough the fire-grid, if encountering fire, see if it spreads\n",
    "                if state[x,y,1] == 1 : \n",
    "                    # with the probability self.prob_spread the fire spreads \n",
    "                    if np.random.rand() < self.prob_spread :\n",
    "                        # a random move , no diagonal moves are allowed\n",
    "                        if np.random.rand() < 0.5 :\n",
    "                            x_step = np.random.randint(2)*2-1\n",
    "                            y_step = 0\n",
    "                        else :\n",
    "                            x_step = 0\n",
    "                            y_step = np.random.randint(2)*2-1\n",
    "                        # if within boundaries\n",
    "                        if (0<= x+x_step < self.size[0]) and (0<= y+y_step < self.size[1]):\n",
    "                            # Check that this square is not allready on fire\n",
    "                            if new_state[x+x_step,y+y_step,1] == 0: \n",
    "                                # LET IT BURN!\n",
    "                                new_state[x+x_step,y+y_step,1] = 1\n",
    "                                did_fire_spread = True\n",
    "                                \n",
    "        \n",
    "        return new_state , did_fire_spread\n",
    "    \n",
    "    \n",
    "    \n",
    "##============ TRAINING AND EXPERIENCE REPLAY ============= \n",
    "  \n",
    "    # Replay implements the training and experience replay. Here we want to show two different implementations\n",
    "    #, one of which is currently commented away in the main loop. The commented version, version 2, we use the\n",
    "    # target network to predict new targets for old states. In version 1, the currently used version,\n",
    "    # we recall the old predictions of the targets. We find that version 1 is more stable\n",
    "    # than version 2 for this problem. But you can try out both versions.\n",
    "    def replay_V1(self, policy_model):\n",
    "        # check if the memory is bigger than the batch size\n",
    "        if len(self.memory) < self.batchSize :\n",
    "            # if not recall whole memory\n",
    "            minibatch = self.memory\n",
    "        else :\n",
    "            # otherwise take a random batch of size batchSize of the memory\n",
    "            minibatch = random.sample(self.memory, self.batchSize)\n",
    "   \n",
    "        # initialize a state and a target batch for training\n",
    "        state_batch = np.zeros((len(minibatch),self.size[0],self.size[1],self.layers))\n",
    "        target_batch = np.zeros((len(minibatch),4))\n",
    "\n",
    "        # Go through memory \n",
    "        i = 0\n",
    "        for (state, q_state, action, reward, next_state, next_q_max, done) in minibatch :\n",
    "            # Version 1:\n",
    "            #target values for network are the same as the output of the network:\n",
    "            target = q_state.reshape(4) \n",
    "            #except for the action where we have an experience that we can use:\n",
    "            new_target = reward         \n",
    "            if not done :\n",
    "                # Version 1:\n",
    "                new_target = reward + self.gamma * next_q_max\n",
    "                \n",
    "            target[action] = new_target\n",
    "            # Put state and target in the training batch\n",
    "            state_batch[i] = state\n",
    "            target_batch[i] = target\n",
    "            i = i + 1\n",
    "   \n",
    "        # Trainp\n",
    "        policy_model.fit(state_batch, target_batch, batch_size = len(minibatch), epochs=1, verbose=1)\n",
    "            \n",
    "\n",
    "    def replay_V2(self, target_model,policy_model):\n",
    "\n",
    "        # check if the memory is bigger than the batch size\n",
    "        if len(self.memory) < self.batchSize :\n",
    "            # if not recall whole memory\n",
    "            minibatch = self.memory\n",
    "        else :\n",
    "            # otherwise take a random batch of size batchSize of the memory\n",
    "            minibatch = random.sample(self.memory, self.batchSize)\n",
    "   \n",
    "        # initialize a state and a target batch for training\n",
    "        state_batch = np.zeros((len(minibatch),self.size[0],self.size[1],self.layers))\n",
    "        target_batch = np.zeros((len(minibatch),4))\n",
    "\n",
    "        # Go through memory \n",
    "        i = 0\n",
    "        for (state, q_state, action, reward, next_state, next_q_max, done) in minibatch :\n",
    "            # Version 2:\n",
    "\n",
    "            target = target_model.predict(state.reshape(1,self.size[0],self.size[1],self.layers)).reshape(4)\n",
    "            new_target = reward\n",
    "            if not done :\n",
    "                # Version 2:\n",
    "                next_q_state = target_model.predict(next_state.reshape(1,self.size[0],self.size[1],self.layers))\n",
    "                new_target = reward + self.gamma *  np.amax(next_q_state)\n",
    "                \n",
    "            target[action] = new_target\n",
    "            # Put state and target in the training batch\n",
    "            state_batch[i] = state\n",
    "            target_batch[i] = target\n",
    "            i = i+1\n",
    "              \n",
    "        # Train\n",
    "        policy_model.fit(state_batch, target_batch, batch_size = len(minibatch), epochs=1, verbose=1)\n",
    " \n",
    "            \n",
    "            \n",
    "##====================FOR DISPLAY==============================    \n",
    "\n",
    "    # Used to display the grid\n",
    "    def make_RGB_grid(self,state,path):\n",
    "        grid_RGB = np.ones((self.size[0],self.size[1],3))*0.7 #\n",
    "        \n",
    "        if path is not None :\n",
    "            for i,location in enumerate(path):\n",
    "                grid_RGB[location[0],location[1],:] = np.array([0,0,0]) # black'P' #player\n",
    "    \n",
    "        for x in range(self.size[0]) : \n",
    "            for y in range(self.size[1]) :\n",
    "            \n",
    "                if state[x,y,2]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([245/255,237/255,48/255]) # Yellow\n",
    "                \n",
    "                if state[x,y,1]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([203/255,32/255,40/255]) # Red '-' #pit    \n",
    "   \n",
    "                if state[x,y,0]==1:\n",
    "                    grid_RGB[x,y,:] = np.array([0/255,254/255,0/255]) # Green '-' #pit    \n",
    "   \n",
    "        return grid_RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to set up and compile the Q-network, similarly to what we did in Homework 2. Input should be the shape of the state matrix, output the 4 action values corresponding to the 4 allowed actions of the agent.\n",
    "N.B. You don't want softmax output, as we want to output Q-values, instead use _linear_ activation. Also, since we're not considering a probability distribution, use _mean square error_ as loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network\n",
    "\n",
    "def setup_network(world) : \n",
    "    # setup network, you need world.size and world.layers for input_shape\n",
    "    # Input 10, 10 , 3\n",
    "    model = Sequential()\n",
    "    # batch, 10, 10, 16\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding='same', activation=\"relu\", input_shape=(world.size[0],world.size[1],world.layers)))\n",
    "    # 10, 10, 16\n",
    "    model.add(Flatten(input_shape=(world.size[0], world.size[1], 16))) # \n",
    "    # 1600, 1\n",
    "    model.add(Dense(4, activation=\"linear\"))\n",
    "    # \n",
    "    \n",
    "    # compile network\n",
    "    model.compile(optimizer='adam', \n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "    # return model \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup network and GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 10, 10, 16)        448       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 6404      \n",
      "=================================================================\n",
      "Total params: 6,852\n",
      "Trainable params: 6,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup GridWorld and Q-networks\n",
    "\n",
    "# Create world from GridWorld\n",
    "world = GridWorld()\n",
    "# Setup network\n",
    "policy_model = setup_network(world)\n",
    "#make a target network as well\n",
    "target_model = setup_network(world)\n",
    "# copy weights from policy to target\n",
    "target_model.set_weights(policy_model.get_weights())\n",
    "\n",
    "# Plot model summary\n",
    "policy_model.summary()\n",
    "\n",
    "#Make state \n",
    "state , player_coordinate = world.make_state(False)\n",
    "\n",
    "# plot it \n",
    "grid_RGB =world.make_RGB_grid(state,None)\n",
    "#\n",
    "fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# We have to invert the x and y axis , go over to numpy array instead\n",
    "plt.imshow(np.swapaxes(np.array(grid_RGB),0,1))\n",
    "#plt.axis('on')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(np.arange(0, world.size[0], dtype=np.int))\n",
    "plt.yticks(np.arange(0, world.size[1], dtype=np.int))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network on model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE YOU NEED TO DEFINE PARAMETER VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup system parameters\n",
    "\n",
    "world.gamma = 0.9 # how much to value the new experience Q(s', a')\n",
    "world.epsilon_decay = 0.9999 #multiplicative factor that reduces epsilon each step, for no reduction use 1\n",
    "world.epsilon = 0.4  #initial value of epsilon \n",
    "world.wind = 0\n",
    "# fire spreading\n",
    "world.prob_spread = 0.5 # \n",
    "\n",
    "#update the target network every \"update_target_network_period game\". Updating target network less often should make\n",
    "#the system more stable, but also convergence slower\n",
    "update_target_network_period = 5\n",
    "\n",
    "#define size of experience replay buffer (how many moves are stored for training) \n",
    "#and batchsize (how many moves from memory buffer are used in each training instance)\n",
    "world.memory_size = 200\n",
    "world.batchSize = 512\n",
    "world.memory = deque(maxlen=world.memory_size)   #The experience replay memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnostics. This is used to print max, min, q-values for states visited in order to get a \n",
    "diagnostic on the Q-network; does it produce reasonable values given the reward scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnostics.  \n",
    "q_max = 0\n",
    "q_min = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop. It should be all set to run if you have defined the network and parameters above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game #: 2999\n",
      "Epsilon :  0.2964\n",
      "Step count : 9\n",
      "End pos [8, 1]\n",
      "Since updated target: Qmin  -33.34457 Qmax 104.44685\n",
      "Train on 200 samples\n",
      "200/200 [==============================] - 0s 30us/sample - loss: 38.0942 - accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "# MAIN LOOP with network\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    step_count=0;\n",
    "    random_start = True\n",
    "    is_wind = True  # Might be true here, but with wind value set to 0 it doesn't generate random move\n",
    "    next_player_coordinate = None\n",
    "    nr_games = 3000  #This is a large number, hopefully it will converge earlier\n",
    "    # loop over games\n",
    "    for games in range(nr_games):\n",
    "\n",
    "        # Display\n",
    "        print(\"Game #: %s\" % (games,))\n",
    "        print(\"Epsilon : %7.4f\" % world.epsilon)     \n",
    "        print(\"Step count : %s\" % step_count) \n",
    "        print(\"End pos %s\" % next_player_coordinate)\n",
    "\n",
    "        # DIAGNOSTICS \n",
    "        print(\"Since updated target: Qmin  %s Qmax %s\" % (q_min,q_max))\n",
    "\n",
    "\n",
    "        # reinitize grid every game will be created at start position\n",
    "        state , player_coordinate = world.make_state(random_start)\n",
    "\n",
    "        step_count=0;\n",
    "        while True :\n",
    "            step_count+=1\n",
    "\n",
    "            # use policy network to get q\n",
    "            q_state = policy_model.predict(state.reshape(1,world.size[0],world.size[1],world.layers))        \n",
    "            # get best action\n",
    "            action = np.argmax(q_state)   \n",
    "\n",
    "            # epsilon greedy\n",
    "            if np.random.rand() < world.epsilon :\n",
    "                # take another action\n",
    "                action=np.random.randint(4)\n",
    "\n",
    "            # make the move\n",
    "            next_state ,next_player_coordinate, reward , done = world.make_move(state,action,player_coordinate,is_wind)\n",
    "\n",
    "            # find max q of the next state using target network\n",
    "            next_q_max = np.amax(target_model.predict(next_state.reshape(1,world.size[0],world.size[1],world.layers)))\n",
    "\n",
    "            # Store in memory\n",
    "            world.memory.extend([(state, q_state, action, reward, next_state, next_q_max, done)])  \n",
    "\n",
    "            # DIAGNOSTICS UPDATE   =============\n",
    "            if q_max < np.amax(q_state):\n",
    "                q_max = np.amax(q_state)\n",
    "            if q_min > np.amin(q_state):\n",
    "                q_min = np.amin(q_state)\n",
    "\n",
    "\n",
    "            #=============================================\n",
    "\n",
    "\n",
    "            # break if done or two many steps taken \n",
    "            if done or (step_count > 400): # 10^2 =100 steps to diffuse through the lattice\n",
    "                break\n",
    "\n",
    "            # update state\n",
    "            state = next_state \n",
    "            player_coordinate = next_player_coordinate \n",
    "\n",
    "            # let fire spread \n",
    "            new_state, fire_spread = world.let_fire_spread(state)\n",
    "            if fire_spread :\n",
    "                state = new_state \n",
    "\n",
    "\n",
    "        # end of game, time to train the network\n",
    "        if len(world.memory) > 0 : # if there is something in the memory\n",
    "            # Version 1\n",
    "            world.replay_V1(policy_model)\n",
    "            # Version 2\n",
    "            #world.replay_V2(target_model,policy_model)\n",
    "            # epsilon decay\n",
    "            if world.epsilon > world.epsilon_min:\n",
    "                world.epsilon *= world.epsilon_decay \n",
    "\n",
    "        # update target network \n",
    "        if (games % update_target_network_period == 0) :\n",
    "            print(\"Update target network\")\n",
    "            # update the weights of the target model\n",
    "            target_model.set_weights(policy_model.get_weights())\n",
    "            #reset diagnostic\n",
    "            q_max = 0\n",
    "            q_min = 0\n",
    "\n",
    "        clear_output(wait=True)    \n",
    "        # end of loop\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the Kernel as you like after a number of games and check progress by using the two plotfunctions below. Restarting the loop will reset the counter but not reset the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\MiniConda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: my_network.h1\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model, here you could save and load Q-networks to compare\n",
    "policy_model.save('my_network.h1')  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "policy_model = load_model('my_network.h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting : State value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# State value function of network, i.e. max over actions of the Q-function\n",
    "\n",
    "# Initialize state, this means that the fire is in the strating position\n",
    "state , player_coordinate = world.make_state(False)\n",
    "\n",
    "# set position to zero\n",
    "state[player_coordinate[0],player_coordinate[1],0] = 0\n",
    "\n",
    "# to plot\n",
    "z= np.zeros((world.size[0],world.size[1]))\n",
    "\n",
    "# Go through all possible position of the player and calculate the value of the best action\n",
    "# according to the network\n",
    "for x in range(world.size[0]) :\n",
    "    for y in range(world.size[1]) :\n",
    "            player_coordinate=[x,y]\n",
    "            state[player_coordinate[0],player_coordinate[1],0] = 1\n",
    "            q_state = policy_model.predict(state.reshape(1,world.size[0],world.size[1],world.layers)).reshape(4)\n",
    "            z[x,y] =q_state.max()\n",
    "            state[x,y,0] = 0\n",
    "        \n",
    "\n",
    "# Plot        \n",
    "plt.figure()\n",
    "fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.imshow(np.swapaxes(z,0,1))\n",
    "plt.colorbar()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(np.arange(0, world.size[0], dtype=np.int))\n",
    "plt.yticks(np.arange(0, world.size[1], dtype=np.int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting : Dynamic play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire spread\n"
     ]
    }
   ],
   "source": [
    "# dynamic game replay\n",
    "\n",
    "for a in range(100):\n",
    "    # fire spreading\n",
    "    world.prob_spread = 0.5\n",
    "    is_wind = False\n",
    "\n",
    "\n",
    "    #get original state  \n",
    "    state, player_coordinate = world.make_state(True)\n",
    "    path=np.array([player_coordinate])\n",
    "    \n",
    "    # setup figure\n",
    "    fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    done = False\n",
    "    count =0\n",
    "    while (not done) and (count <20) :\n",
    "        count = count + 1\n",
    "        \n",
    "        # plot it \n",
    "        plot_grid = world.make_RGB_grid(state,path)\n",
    "        plt.imshow(np.swapaxes(np.array(plot_grid),0,1))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xticks(np.arange(0, world.size[0], dtype=np.int))\n",
    "        plt.yticks(np.arange(0, world.size[1], dtype=np.int))\n",
    "        # clear figure and wait\n",
    "        clear_output(wait=True)\n",
    "        # time.sleep(1)\n",
    "        display(fig)\n",
    "\n",
    "        # find action\n",
    "        q_state = policy_model.predict(state.reshape(1,world.size[0],world.size[1],world.layers))\n",
    "        # get best action, no epsilon greedy\n",
    "        action = np.argmax(q_state)      \n",
    "        # make the move\n",
    "        next_state ,next_player_coordinate, reward , done = world.make_move(state,action,player_coordinate,is_wind)\n",
    "\n",
    "        # update state \n",
    "        state = next_state\n",
    "        player_coordinate = next_player_coordinate\n",
    "\n",
    "        if not done :\n",
    "            path=np.append(path,[player_coordinate],axis = 0)\n",
    "\n",
    "        # let fire spread\n",
    "        new_state, fire_spread = world.let_fire_spread(state)\n",
    "        if fire_spread :\n",
    "            print(\"Fire spread\")\n",
    "            state = new_state \n",
    "\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
